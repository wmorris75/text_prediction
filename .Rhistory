summary(fit3)$coeff
fit5<-lm(mpg ~ am + hp + wt + gear + carb, data = mtcars)
summary(fit5)$coeff
fit6<-lm(mpg ~ am + hp + wt + gear + carb + disp, data = mtcars)
summary(fit6)$coeff
fit7<-lm(mpg ~ am + hp + wt + gear + carb + disp + drat, data = mtcars)
summary(fit7)$coeff
fit7<-lm(mpg ~ am + hp + wt + gear + carb + disp + drat + vs, data = mtcars)
summary(fit7)$coeff
fit7<-lm(mpg ~ am + hp + wt + gear + carb + disp + drat + vs + qsec, data = mtcars)
summary(fit7)$coeff
fit1<-lm(mpg ~ am, data = mtcars)
fit2<-lm(mpg ~ am + hp, data = mtcars )
fit3<-lm(mpg ~ am + hp + wt, data = mtcars)
fit4<-lm(mpg ~ am + hp + wt + gear, data = mtcars)
fit5<-lm(mpg ~ am + hp + wt + gear + carb, data = mtcars)
fit6<-lm(mpg ~ am + hp + wt + gear + carb + disp, data = mtcars)
fit7<-lm(mpg ~ am + hp + wt + gear + carb + disp + drat, data = mtcars)
fit8<-lm(mpg ~ ., data = mtcars)
rbind(sum(fit1)$coeff[2,], sum(fit2)$coeff[2,], sum(fit3)$coeff[2,],sum(fit4)$coeff[2,],sum(fit5)$coeff[2,],sum(fit6)$coeff[2,], sum(fit7)$coeff[2,],sum(fit8)$coeff[2,])
rbind(summary(fit1)$coeff[2,], summary(fit2)$coeff[2,], summary(fit3)$coeff[2,],summary(fit4)$coeff[2,],summary(fit5)$coeff[2,],summary(fit6)$coeff[2,], summary(fit7)$coeff[2,],summary(fit8)$coeff[2,])
lm(mpg ~ ., data = mtcars)
s<-lm(mpg ~ ., data = mtcars)
summary(s)$coeff
fit1<-lm(mpg ~ am, data = mtcars)
fit2<-lm(mpg ~ am + hp, data = mtcars )
fit3<-lm(mpg ~ am + hp + wt, data = mtcars)
fit4<-lm(mpg ~ am + hp + wt + gear, data = mtcars)
fit5<-lm(mpg ~ am + hp + wt + gear + carb, data = mtcars)
fit6<-lm(mpg ~ am + hp + wt + gear + carb + disp, data = mtcars)
fit7<-lm(mpg ~ am + hp + wt + gear + carb + disp + drat, data = mtcars)
fit8<-lm(mpg ~ factor(am) + ., data = mtcars)
rbind(summary(fit1)$coeff[2,], summary(fit2)$coeff[2,], summary(fit3)$coeff[2,],summary(fit4)$coeff[2,],summary(fit5)$coeff[2,],summary(fit6)$coeff[2,], summary(fit7)$coeff[2,],summary(fit8)$coeff[2,])
anova(fit1, fit2, fit3, fit4, fit5, fit6, fit7, fit8)
f<-lm(mpg ~ am + hp + wt + disp, data = mtcars)
summary(f)$coeff
summary(fit8)$coeff
rbind(summary(fit1)$coeff[2,], summary(fit2)$coeff[2,], summary(fit3)$coeff[2,],summary(fit4)$coeff[2,],summary(fit5)$coeff[2,],summary(fit6)$coeff[2,], summary(fit7)$coeff[2,],summary(fit8)$coeff[2,])
compare_fit<-rbind(summary(fit1)$coeff[2,], summary(fit2)$coeff[2,], summary(fit3)$coeff[2,],summary(fit4)$coeff[2,],summary(fit5)$coeff[2,],summary(fit6)$coeff[2,], summary(fit7)$coeff[2,],summary(fit8)$coeff[2,])
compare_fit
lm(mpg ~ am + wt, data = mtcars )
summary(lm(mpg ~ am + wt, data = mtcars ))$coeff
summary(lm(mpg ~ am + hp + wt, data = mtcars))$coeff
anova(fit1, fit2, fit3, fit4, fit5, fit6, fit7, fit8)
compare_fit
head(mtcars)
summary(fit3)$coeff
summary(fit3)$coeff
summary(fit8)$coeff
summary(fit4)$coeff
summary(fit5)$coeff
head(mtcars)
g = 2.084 + (-0.0375*110) + (2.620 *  -2.87857541) + 34
g
tail(mtcars)
2.084 + (-0.0375*91) + (2.14 *  -2.87857541) + 34
(-0.0375*91) + (2.14 *  -2.87857541) + 34
summary(fit3)$coeff
fit1<-lm(mpg ~ factor(am), data = mtcars)
fit2<-lm(mpg ~ factor(am) + hp, data = mtcars )
fit3<-lm(mpg ~ factor(am) + hp + wt, data = mtcars)
fit4<-lm(mpg ~ factor(am) + hp + wt + gear, data = mtcars)
fit5<-lm(mpg ~ factor(am) + hp + wt + gear + carb, data = mtcars)
fit6<-lm(mpg ~ factor(am) + hp + wt + gear + carb + disp, data = mtcars)
fit7<-lm(mpg ~ factor(am) + hp + wt + gear + carb + disp + drat, data = mtcars)
fit8<-lm(mpg ~ factor(am) + ., data = mtcars)
summary(fit3)$coeff
predict(fit3, newdata=data.frame(wt=2.68, hp= 99), interval="confidence")
predict(fit3, newdata=data.frame(wt=2.68, hp= 99, factor(am)), interval="confidence")
predict(fit3,  factor(am), newdata=data.frame(wt=2.68, hp= 99), interval="confidence", data = mtcars)
predict(fit3, newdata=data.frame(wt=2.68, hp= 99), interval="confidence", data = mtcars)
predict(fit3, newdata=data.frame(wt = mtcars$wt, hp = mtcars$hp), interval="confidence", data = mtcars)
predict(fit3)
mtcars
(-0.0375*230) + (5.35 *  -2.87857541) + 34
(-0.0375*230) + (5.35 *  -2.87857541) + 34 + 2.08
predict(fit3, newdata=data.frame(c (wt = mtcars$wt, hp = mtcars$hp)), interval="confidence", data = mtcars)
predict(fit3, newdata=data.frame(c (wt = mtcars$wt, hp = mtcars$hp, mtcars$am)), interval="confidence", data = mtcars)
predict(fit3, newdata=data.frame(c (wt = mtcars$wt, hp = mtcars$hp, am = mtcars$am)), interval="confidence", data = mtcars)
predict(fit3, newdata=data.frame(c (wt = mtcars$wt, hp = mtcars$hp, am = mtcars$am)), interval="confidence")
predict(fit3, newdata=data.frame(c(wt = mtcars$wt, hp = mtcars$hp, am = as.factor(mtcars$am))), interval="confidence")
fit3
resid(fit3)
pairs(mtcars)
plot(predict(fit8), resid(fit8), pch = '.')
fit8
plot(predict(fit8), resid(fit8), pch = '.')
resid(fit8)
predict(fit8)
plot(predict(fit8), resid(fit8), pch = '.', type = 'point')
plot(predict(fit8), resid(fit8), pch = '.', type = 'points')
plot(predict(fit8), resid(fit8), pch = '.', type = 'p')
plot(predict(fit8), resid(fit8), pch = '.', type = p)
plot(predict(fit8), resid(fit8), pch = '.')
plot(predict(fit8), resid(fit8))
plot(predict(fit3), resid(fit3))
plot(predict(fit2), resid(fit2))
plot(predict(fit1), resid(fit1))
plot(predict(fit4), resid(fit4))
plot(predict(fit3), resid(fit3))
confint(fit3)
confint(fit4)
confint(fit8)
confint(fit1)
confint(fit2)
resid(fit3)
predict(resid(fit3))
plot(fit3)
plot(fit8)
plot(fit3)
par(mfrow = c(2, 2))
plot(fit3)
confint(fit2)
confint(fit3)
compare_fit
predict(fit3)
head(mtcars)
summary(fit3)
confint(fit3)
confint(fit8)
confint(fit4)
confint(fit5)
confint(fit6)
confint(fit3)
confint(fit8)
plot(predict(mtcars), predict(fit3))
plot(resid(fit3), predict(fit3))
plot(resid(fit4), predict(fit4))
plot(resid(fit8), predict(fit8))
plot(resid(fit2), predict(fit2))
plot(resid(fit5), predict(fit5))
par(mfrow = c(2, 2))
plot(fit3)
plot(fit8)
plot(resid(fit3), predict(fit3))
abline(a = 0)
abline(x = 0)
plot(predict(fit3), resid(fit3))
line(y = 0, colors = 'red')
line(x, y = 0, colors = 'red')
par(mfrow = c(2, 2))
plot(predict(fit3), resid(fit3))
abline(yintercept = 0)
abline(h = 0, v = 0, col = "gray60")
abline(h = 2, v = 0, col = "gray60")
abline(h = -2, v = 0, col = "gray60")
plot(predict(fit2), resid(fit2))
abline(h = 2, v = 0, col = "gray60")
abline(h = -2, v = 0, col = "gray60")
abline(h = 0, v = 0, col = "gray60")
plot(predict(fit6), resid(fit6))
abline(h = -2, v = 0, col = "gray60")
abline(h = 2, v = 0, col = "gray60")
abline(h = 0, v = 0, col = "gray60")
plot(predict(fit8), resid(fit8))
abline(h = 0, v = 0, col = "gray60")
abline(h = 2, v = 0, col = "gray60")
abline(h = -2, v = 0, col = "gray60")
abline(h = -2, v = 0, col = "red")
plot(predict(fit3), resid(fit3))
plot(predict(fit3), resid(fit3))
confint(resid(fit3))
confint((fit3))
summary(fit3)
summary(fit4)
summary(fit8)
f<-mtcars$mpg
predict(fit3)
class(predict(fit3))
predict(fit3)[1]
predict(fit3)[1,2]
predict(fit3)[1]
plot(f, predict(fit3))
abline()
line(f, predict(fit3))
abline(f ~ predict(fit3))
abline(f ~ predict(fit3), col = "red")
lines(lowess(f ,predict(fit3)), col="blue")
line(f, predict(fit8))
plot(f, predict(fit8))
line(f, predict(fit8))
abline(f ~ predict(fit8), col = "red")
lines(lowess(f ,predict(fit8)), col="blue")
summary(fit3)
confint(fit3)
compare_fit
devtools::install_github("rstudio/rmarkdown")
#Linear Discriminant Model
#Boosting
confusionMatrix(gbmprediction, testing$classe)$overall[1]
#RPart
confusionMatrix(rPartprediction, testing$classe)$overall[1]
#Random Forest
confusionMatrix(rfprediction, testing$classe)$overall[1]
#Linear Discriminant Model
confusionMatrix(ldaprediction, testing$classe)$overall[1]
#Naive Bayes
confusionMatrix(nbprediction, testing$classe)$overall[1]
# Chunk 1
library(caret)
library(ggplot2)
library(MASS)
# Chunk 2
training_data<-read.csv("pml-training.csv")
testing_data<-read.csv("pml-testing.csv")
train_data<-training_data[(grep("accel|classe", names(training_data)))]
test_data<-testing_data[(grep("accel|classe", names(testing_data)))]
# Chunk 3
names(train_data)
str(train_data)
# Chunk 4
#variables to be removed from data
rm_names<-c("var_total_accel_belt", "var_accel_arm", "var_accel_dumbbell", "var_accel_forearm")
new_train_data<-train_data[!names(train_data) %in% rm_names]
new_test_data<-test_data[!names(test_data) %in% rm_names]
str(new_train_data)
# Chunk 5
set.seed(323)
new_train_data$classe <-as.factor(new_train_data$classe)
inTrain <- createDataPartition(new_train_data$classe, p=0.7, list=FALSE)
training<-new_train_data[inTrain,]
testing<-new_train_data[-inTrain,]
# Chunk 6
set.seed(323)
gbmfit<-train(classe ~ ., data=training, method="gbm")
gbmprediction<-predict(gbmfit, testing)
#Accuracy
confusionMatrix(gbmprediction, testing$classe)   #0.80 accuracy
# Chunk 7
set.seed(323)
rpartFit<-train(classe ~ ., data=training, method="rpart")
rPartprediction<-predict(rpartFit, testing)
#Accuracy
confusionMatrix(rPartprediction, testing$classe)   #0.42  accuracy
#Used rpart as prediction model; 0.42% accuracy
# Chunk 8
set.seed(323)
rfFit<-train(classe ~ ., data=training, method="rf")
rfprediction<-predict(rfFit, testing)
#Accuracy
confusionMatrix(rfprediction, testing$classe)   #0.9475  accuracy
#Used rf as prediction model; 0.42% accuracy
# Chunk 9
set.seed(323)
ldaFit<-train(classe ~ ., data=training, method="lda")
ldaprediction<-predict(ldaFit, testing)
#Accuracy
confusionMatrix(ldaprediction, testing$classe)   #0.  accuracy
#Used lda as prediction model; 0.51% accuracy
# Chunk 10
set.seed(323)
nbFit<-train(classe ~ ., data=training, method="nb")
nbprediction<-predict(nbFit, testing)
#Accuracy
confusionMatrix(nbprediction, testing$classe)   #0.  accuracy
#Used nb as prediction model; 0.57% accuracy
# Chunk 11
#Boosting
confusionMatrix(gbmprediction, testing$classe)$overall[1]
#RPart
confusionMatrix(rPartprediction, testing$classe)$overall[1]
#Random Forest
confusionMatrix(rfprediction, testing$classe)$overall[1]
#Linear Discriminant Model
confusionMatrix(ldaprediction, testing$classe)$overall[1]
#Naive Bayes
confusionMatrix(nbprediction, testing$classe)$overall[1]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength,
p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[­inTrain,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength,
p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[­inTrain,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength,
p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[inTrain,]
nbFit<-train(CompressiveStrength ~ ., data=training, method="rpart")
head(data("concrete"))
head(concrete)
bprediction<-predict(nbFit, 79.99)
nbFit<-train(CompressiveStrength ~ ., data=training, method="rpart")
nbFit
bprediction<-predict(nbFit, testing)
bprediction
bprediction<-predict(nbFit, 79.9)
bprediction<-predict(nbFit, testing)
bprediction
testing
testing[1,]
bprediction<-predict(nbFit, testing[1,])
bprediction
nbFit<-train(CompressiveStrength ~ FineAggregate + CoarseAggregate + FlyAsh, data=training, method="rpart")
bprediction<-predict(nbFit, testing)
nbFit<-train(CompressiveStrength ~ FineAggregate, data=training, method="rpart")
bprediction<-predict(nbFit, 0.56)
bprediction<-predict(nbFit, testing$FineAggregate)
bprediction<-predict(nbFit, testing)
testing
class(testing)
bprediction<-predict(nbFit$CompressiveStrength, testing)
bprediction<-predict(nbFit, data.frame(c(0.56))
)
bprediction<-predict(nbFit, data.frame(c(0.56)))
library(NLP)
library(tm)
library(tau)
blog <- readLines('blog_sample.txt')
news <- readLines('news_sample.txt')
twitter <- readLines('twitter_sample.txt')
length(blog)
length(news)
length(twitter)
library(NLP)
library(tm)
library(ggplot2)
library(tau)
blog <- readLines('en_US.blogs.txt')
news <- readLines('en_US.news.txt')
twitter <- readLines('en_US.twitter.txt')
blogsample<-sample(blog, size=0.1*length(blog))
newsample<-sample(news, size=0.05*length(news))
twittersample<-sample(twitter, size=0.05*length(twitter))
twitter_txt<-c(blogsample, newsample, twittersample)
length(blog)
length(news)
length(twitter_txt)
library(shiny)
setwd("~/Documents/DataScience/Capstone/final/text_product")
runApp()
runApp()
library(NLP)
library(tm)
library(ggplot2)
library(tau)
read_files <- function(){
blog <- readLines('en_US.blogs.txt')
news <- readLines('en_US.news.txt')
twitter <- readLines('en_US.twitter.txt')
blogsample<-sample(blog, size=0.1*length(blog))
newsample<-sample(news, size=0.05*length(news))
twittersample<-sample(twitter, size=0.05*length(twitter))
twitter_txt<-c(blogsample, newsample, twittersample)
return (twitter_txt)
}
#Returns sorted n grams after words have been processed and cleaned
get_n_grams<-function(content.words, n){
# the NLP function "ngrams" returns a list of n pairs of words.
content.bigrams = vapply(ngrams(content.words, n), paste, "", collapse = " ")
# we count them using xtabs,
# and put the result into a data frame.
content.bigram.counts = as.data.frame(xtabs(~content.bigrams))
filename <- paste(as.character(n), "_grams.txt", sep='')
content.bigram.counts<-content.bigram.counts[order(content.bigram.counts$Freq, decreasing = TRUE),]
write.table(content.bigram.counts, filename)
return (content.bigram.counts)
}
predict_text<-function(text, get_3_grams_count, get_2_grams_count){
max_count_grams = get_3_grams_count[which(get_3_grams_count$Freq == max(get_3_grams_count$Freq)),]
if (length(max_count_grams$Freq) < 5){
#No 3 grams present. Use 2 grams instead
max_count_grams = get_2_grams_count[which (get_2_grams_count$Freq == max(get_2_grams_count$Freq)),]
}
for(i in word_list){
grams_2_predict <- paste("^would", i, sep=" ")
get_2_grams_count <- grams_table[grep(grams_2_predict, grams_table$content.bigrams),]
max_count_grams = get_2_grams_count[which (get_2_grams_count$Freq == max(get_2_grams_count$Freq)),]
print(max_count_grams)
}
#if gram_3_predict
if(length(max_count_grams$content.bigram) > 0){
predicted_word = strsplit(max_count_grams$content.bigram, " ", fixed = TRUE)[[1]]
return (predicted_word[length(predicted_word)])
}
return(words[length(words)])
}
twitter_txt = read_files()
content.str = paste(twitter_txt, collapse = " ")
content.str = gsub("[^[:alpha:] ]", "", content.str)
content.str = gsub("[ ]+", " ", content.str)
# we use the package TM to lowercase everything
# and to remove punctuation.
# Here is how you turn a single text, given as a string,
# into a tm object:
content.corpus = Corpus(VectorSource(content.str))
content.corpus = tm_map(content.corpus, tolower)
content.corpus = tm_map(content.corpus, removePunctuation, preserve_intra_word_dashes = FALSE)
#Remove all the numbers from string
content.corpus = tm_map(content.corpus, removeNumbers)
# Now change this tm object back into a long string,
# lowercased and minus the punctuation
cleaned.content.str = as.character(content.corpus)[1]
# split into words
content.words = strsplit(cleaned.content.str, " ", fixed = T)[[1]]
twitter_2_grams <- get_n_grams(content.words, 2)
twitter_3_grams <- get_n_grams(content.words, 3)
read_files <- function(){
blog <- readLines('../en_US.blogs.txt')
news <- readLines('../en_US.news.txt')
twitter <- readLines('../en_US.twitter.txt')
blogsample<-sample(blog, size=0.1*length(blog))
newsample<-sample(news, size=0.05*length(news))
twittersample<-sample(twitter, size=0.05*length(twitter))
twitter_txt<-c(blogsample, newsample, twittersample)
return (twitter_txt)
}
twitter_txt = read_files()
content.str = paste(twitter_txt, collapse = " ")
content.str = gsub("[^[:alpha:] ]", "", content.str)
content.str = gsub("[ ]+", " ", content.str)
# we use the package TM to lowercase everything
# and to remove punctuation.
# Here is how you turn a single text, given as a string,
# into a tm object:
content.corpus = Corpus(VectorSource(content.str))
content.corpus = tm_map(content.corpus, tolower)
content.corpus = tm_map(content.corpus, removePunctuation, preserve_intra_word_dashes = FALSE)
#Remove all the numbers from string
content.corpus = tm_map(content.corpus, removeNumbers)
# Now change this tm object back into a long string,
# lowercased and minus the punctuation
cleaned.content.str = as.character(content.corpus)[1]
# split into words
content.words = strsplit(cleaned.content.str, " ", fixed = T)[[1]]
twitter_2_grams <- get_n_grams(content.words, 2)
twitter_3_grams <- get_n_grams(content.words, 3)
twitter <- readLines('../en_US/en_US.twitter.tx
t)
''
twitter <- readLines('../en_US/en_US.twitter.txt')
read_files <- function(){
blog <- readLines('../en_US/en_US.blogs.txt')
news <- readLines('../en_US/en_US.news.txt')
twitter <- readLines('../en_US/en_US.twitter.txt')
blogsample<-sample(blog, size=0.1*length(blog))
newsample<-sample(news, size=0.05*length(news))
twittersample<-sample(twitter, size=0.05*length(twitter))
twitter_txt<-c(blogsample, newsample, twittersample)
return (twitter_txt)
}
runApp()
runApp()
runApp()
runApp()
runApp('text_product')
runApp('../text_product')
runApp('text_product')
runApp('../text_product')
text = "I am going to"
content = gsub("[^[:alnum:] ]", "", text)
content = gsub("[ ]+", " ", content)
words = strsplit(content, " ", fixed = T)[[1]]
#if 3 grams take teh last 2 words
words_3 = paste(words[(length(words)-1)], words[length(words)], sep=" ")
grams_3_predict = paste("^", words_3, sep='')
grams_table <- read.table('3_grams.txt', stringsAsFactors = FALSE)
get_3_grams_count <- grams_table[grep(grams_3_predict, grams_table$content.bigrams),]
grams_2_predict = paste('^', words[length(words)], sep='')
grams_table <- read.table('2_grams.txt', stringsAsFactors = FALSE)
get_2_grams_count <- grams_table[grep(grams_2_predict, grams_table$content.bigrams),]
word_predicted<-predict_text(text, get_3_grams_count, get_2_grams_count)
text = "I am going to"
content = gsub("[^[:alnum:] ]", "", text)
content = gsub("[ ]+", " ", content)
words = strsplit(content, " ", fixed = T)[[1]]
#if 3 grams take teh last 2 words
words_3 = paste(words[(length(words)-1)], words[length(words)], sep=" ")
grams_3_predict = paste("^", words_3, sep='')
grams_table <- read.table('3_grams.txt', stringsAsFactors = FALSE)
get_3_grams_count <- grams_table[grep(grams_3_predict, grams_table$content.bigrams),]
grams_2_predict = paste('^', words[length(words)], sep='')
grams_table <- read.table('2_grams.txt', stringsAsFactors = FALSE)
get_2_grams_count <- grams_table[grep(grams_2_predict, grams_table$content.bigrams),]
word_predicted<-predict_text(text, get_3_grams_count, get_2_grams_count)
grams_table <- read.table('3_grams.txt', stringsAsFactors = FALSE)
get_3_grams_count <- grams_table[grep(grams_3_predict, grams_table$content.bigrams),]
grams_table <- read.table('3_grams.txt', stringsAsFactors = FALSE)
get_3_grams_count <- grams_table[grep(grams_3_predict, grams_table$content.bigrams),]
twitter_txt<-c(blogsample, newsample, twittersample)
twitter_txt<-read_files()
content.str = paste(twitter_txt, collapse = " ")
content.str = gsub("[^[:alpha:] ]", "", content.str)
content.str = gsub("[ ]+", " ", content.str)
# we use the package TM to lowercase everything
# and to remove punctuation.
# Here is how you turn a single text, given as a string,
# into a tm object:
content.corpus = Corpus(VectorSource(content.str))
content.corpus = tm_map(content.corpus, tolower)
content.corpus = tm_map(content.corpus, removePunctuation, preserve_intra_word_dashes = FALSE)
#Remove all the numbers from string
content.corpus = tm_map(content.corpus, removeNumbers)
# Now change this tm object back into a long string,
# lowercased and minus the punctuation
cleaned.content.str = as.character(content.corpus)[1]
# split into words
content.words = strsplit(cleaned.content.str, " ", fixed = T)[[1]]
twitter_4_grams <- get_n_grams(content.words, 4)
